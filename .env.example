# AI Provider Configuration
# ========================
# Options: 'ollama' (free, local) or 'anthropic' (paid API)
# Default: ollama (if no ANTHROPIC_API_KEY is set)
AI_PROVIDER=ollama

# Ollama settings (free, runs locally)
# Install: https://ollama.ai
# Start: ollama serve
# Pull model: ollama pull llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Anthropic API Key (only if using anthropic provider)
ANTHROPIC_API_KEY=

# Folder to watch for new transcripts
WATCH_FOLDER=./transcripts

# Server port
PORT=3001

# Database path
DB_PATH=./data/prism.db
